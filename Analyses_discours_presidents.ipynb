{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **PROJET TAL** : Classification de morceaux de discours mélangés appartenants à 2 anciens présidents de la République française : François Mitterand ou Jacques Chirac\n",
    "#### **Auteurs** : Mélodie FLEURY, Matéo PETITET \n",
    "#### **Matière** : UE TAL par Vincent GUIGUE - 3A IODAA AgroParisTech\n",
    "#### **Date de rendue** : 12 février 2025\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectifs\n",
    "Le but de ce projet est de déterminer à partir d'une liste de phrases si ces dernières ont été prononcées par François Mitterand ou par Jacques Chirac, deux anciens présidents de la République française. Dans ce notebook, différentes méthodes d'apprentissage vont être testées pour évaluer et comparer leur performance de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des librairies nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import optuna\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "#Installation librairie ollama si pas installé\n",
    "%pip install ollama\n",
    "import ollama\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les fonctions utilisées dans ce notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pres(fname):\n",
    "    \"\"\"\n",
    "    Chargement des données en séparant les labels (variable \"alllabs\") des morceaux de discours des 2 anciens présidents (variable \"alltxts\") dans le fichier\n",
    "\n",
    "    Args:\n",
    "        fname (str): chemin du fichier des données d'entraînement et de test\n",
    "\n",
    "    Returns: \n",
    "        alltxts (str): morceaux de discours des 2 anciens président du document\n",
    "        alllabs (int): label associé à chaque morceau de discours = Si 0 alors la phrase provenait de Mr Mitterand sinon de Mr Chirac\n",
    "    \"\"\"\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt).strip()\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(0)\n",
    "        else: \n",
    "            alllabs.append(1)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Applique des pré-traitements sur un fichier textuel (\"text\"). Par exemple,\n",
    "    la conversion des mots en minuscule, la suppression des nombres, de la\n",
    "    ponctuation et des stopwords.\n",
    "\n",
    "    Args: \n",
    "        text (str): Texte brute en entrée\n",
    "\n",
    "    Returns: \n",
    "        text (str) : Texte pré-traité en sortie\n",
    "    \"\"\"\n",
    "    # Conversion en minuscules\n",
    "    text = text.lower().strip()\n",
    "    # Suppression des nombres\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    # Suppression des ponctuations\n",
    "    punc = string.punctuation  \n",
    "    punc += '\\n\\r\\t'\n",
    "    text = text.translate(str.maketrans(punc, ' ' * len(punc)))\n",
    "    # Suppression des stopwords\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Rejoindre les tokens en une chaîne\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Classe permettant de structurer les morceaux de discours et les labels pour qu'ils soient plus facilement utilisées par un modèle Pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, texts: list, labels):\n",
    "        self.labels = labels\n",
    "        self.phrasesnum = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.phrasesnum[i], torch.tensor(self.labels[i])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Personnalise le traitement des données sous forme de batch avant d'être entraîné dans un modèle\n",
    "\n",
    "    Args:\n",
    "        batch (tuple): Liste d'échantillon de morceaux de discours (\"sequences\") et de labels de l'ancien président associés\n",
    "\n",
    "    Returns:\n",
    "        padded_sequences (Tensor): Tensor des séquences de longueur uniforme après padding\n",
    "        torch.tensor(lengths) (Tensor): Tensor contenant les longueurs originales des séquences\n",
    "        torch.tensor(labels) (Tensor): Tensor contenant les étiquettes du batch\n",
    "    \"\"\"\n",
    "    #Séparation des séquences et des labels sous la forme de tuples\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    #permet de mettre toutes les séquences du batch à la même longueur en ajoutant des zéros\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=False)\n",
    "    return padded_sequences, torch.tensor(lengths), torch.tensor(labels)\n",
    "\n",
    "def generate_sinusoidal_embeddings(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Génération des embeddings positionnels basés sur des sinusoïdes (sine et cosine) pour permettre au modèle de connaître l'ordre des tokens dans une séquence\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): Longueur de la séquence\n",
    "        d_model (int): Dimension de l'espace des embeddings\n",
    "\n",
    "    Returns:\n",
    "        : Embedding positionnel sinusoïdal\n",
    "    \"\"\"\n",
    "    #Creation d'un tensor avec des entiers de 0 à seq_len -1\n",
    "    position = torch.arange(seq_len).unsqueeze(1) #Ajout d'une dimension supplémentaire afin de créer une colonne de positions\n",
    "    #Génération d'un tensor avec les valeurs appropriées à chaque dimension de d_model\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    #Tensor vide\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    #Application de la fonction sinus aux indices impairs des dimensions\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    #Application de la fonction cosinus aux indices pairs des dimensions\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def accuracy(yhat,y):\n",
    "    \"\"\"\n",
    "    Calcul de la métrique d'évaluation des modèles avec le dataset de test\n",
    "\n",
    "    Args:\n",
    "        yhat (int): label prédit par le modèle\n",
    "        y (int): label vrai\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Accuracy soit la moyenne de bonnes prédictions\n",
    "    \"\"\"\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).float().mean()\n",
    "\n",
    "def train(model,epochs,train_loader,test_loader):\n",
    "    \"\"\"\n",
    "    Entraînement du Transformers et récupération des valeurs d'accuracy pour le jeu d'entraînement et de test\n",
    "\n",
    "    Args:\n",
    "        model: modèle de Transformers\n",
    "        epochs (int): nombre d'itération de descente de gradient\n",
    "        train_loader: Jeu d'entraînement configuré par le DataLoader\n",
    "        test_loader: Jeu de test configuré par le DataLoader\n",
    "\n",
    "    Returns:\n",
    "        list: Listes des valeurs d'accuracy du jeu de donnée d'entraînement et de test\n",
    "    \"\"\"\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=5e-4)    # choix optimizer\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()                            # choix loss\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x, lengths, y in train_loader:                            # boucle sur les batchs\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)                # y doit être un tensor (pas un int)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)                             # attention, il peut y avoir un batch + petit (le dernier)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "\n",
    "        # Calcul de l'accuracy moyenne pour le jeu d'entraînement\n",
    "        train_accuracy = round((cumacc / count).item(),2)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x, lengths, y in test_loader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                \n",
    "                # Calcul de l'accuracy moyenne pour le jeu de test\n",
    "                test_accuracy = round((cumacc / count).item(),2)\n",
    "                test_accuracies.append(test_accuracy)  # Ajout de l'accuracy test à la liste\n",
    "                \n",
    "    print(train_accuracies, test_accuracies)\n",
    "    return train_accuracies, test_accuracies\n",
    "\n",
    "#classification zero-shot\n",
    "def classification_zero_shot(exemple):\n",
    "    \"\"\"\n",
    "    Classification en mode \"zero shot\" utilisant le modèle Qwen\n",
    "\n",
    "    Args:\n",
    "        exemple (str): Phrase à associer au bon ancien président français qui l'a déjà énoncé\n",
    "\n",
    "    Returns:\n",
    "        str: Output du modèle Qwen\n",
    "    \"\"\"\n",
    "    prompt = f\"You are in a quizz. Here is a quote which has been prononced by an old french president, either Mitterand or Chirac. To win, you have to guess which of them said it sentence using only one word : 'Mitterand' or 'Chirac'. To win you can only say one word. Here is the quote :\\n\\n{exemple}\"\n",
    "    reponse = ollama.generate(model='qwen2.5:0.5b',prompt=prompt)\n",
    "    return reponse\n",
    "\n",
    "#classification few-shot\n",
    "def classification_few_shot(exemple):\n",
    "    \"\"\"\n",
    "    Classification en mode \"few shot\" utilisant le modèle Qwen\n",
    "\n",
    "    Args:\n",
    "        exemple (str): Phrase à associer au bon ancien président français qui l'a déjà énoncé\n",
    "\n",
    "    Returns:\n",
    "        str: Output du modèle Qwen\n",
    "    \"\"\"\n",
    "    #Petit jeu d'entraînement pour le modèle\n",
    "    txt = [\"Quand je dis chers amis, il ne s'agit pas là d'une formule diplomatique, mais de l'expression de ce que je ressens.\",\n",
    "           \"A Brazzaville, que l'Afrique de demain se dessine.\",\n",
    "           \"Moi je suis du côté de ceux qui pensent qu'il vaut mieux répartir plus justement l'effort et le produit de la nation.\",\n",
    "           \"Songez que la femme mariée a eu beaucoup de peine à acquérir un statut juridique conforme à l'idée que nous nous faisons des rapports entre les femmes et les hommes.\"]\n",
    "    \n",
    "    prompt = f\"These texts are in the category 'Chirac': \\n{txt[0]}\\n and \\n{txt[1]}.\\n\\nThese texts are in the category 'Mitterand': \\n{txt[2]}\\n and \\n{txt[-1]}.\\n\\n Give the category of the following text in only one word: 'Chirac' or 'Mitterand' :\\n\\n{exemple}\"\n",
    "    reponse = ollama.generate(model='qwen2.5:0.5b',prompt=prompt)\n",
    "    return reponse\n",
    "\n",
    "#évaluation des performances de Qwen\n",
    "def test_perf(dataset,methode,labels_vrais):\n",
    "    \"\"\"Évaluation de la bonne classification de chaque phrase à l'ancien président de la République qui l'avait énoncé via l'usage du modèle Qwen\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Liste de phrases énoncés soit par Mr François Mitterand soit par Mr Jacques Chirac\n",
    "        methode (str): choix de la méthode \"zero shot\" ou \"few shot\"\n",
    "        labels_vrais (int): liste des labels rééls à prédire associé aux anciens président français\n",
    "\n",
    "    Returns:\n",
    "       int: nombre de phrases bien classées, mal classées ou non classées par le modèle\n",
    "    \"\"\"\n",
    "    # Shuffle two lists with same order\n",
    "    temp = list(zip(dataset, labels_vrais))\n",
    "    random.shuffle(temp)\n",
    "    dataset, labels_vrais = zip(*temp)\n",
    "    # res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "    dataset, labels_vrais = list(dataset), list(labels_vrais)\n",
    "\n",
    "    ok, pb_rep, nul = 0, 0, 0\n",
    "\n",
    "    pattern_mitterand = re.compile(r\"(?i)\\s*mitterand\\s*[\\W]*\")\n",
    "    pattern_chirac = re.compile(r\"(?i)\\s*chirac\\s*[\\W]*\")\n",
    "    #(?i) : Active l'option d'insensibilité à la casse (case insensitive), ce qui signifie que \"Mitterand\" et \"mitterand\" seront tous deux reconnus.\n",
    "    #\\s* : Correspond à zéro ou plusieurs espaces avant et après le mot \"Mitterand\".\n",
    "    #\\W : correspond à tout caractère non alphanumérique.\n",
    "\n",
    "    for k in range(len(dataset)):\n",
    "        response = methode(dataset[k]).response\n",
    "        if (pattern_mitterand.match(response) and labels_vrais[k] == 0) or (pattern_chirac.match(response) and labels_vrais[k] == 1):\n",
    "            ok += 1\n",
    "        elif not pattern_mitterand.match(response) and not pattern_chirac.match(response):\n",
    "            pb_rep += 1\n",
    "        else:\n",
    "            nul += 1\n",
    "\n",
    "    return ok, pb_rep, nul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHARGEMENT DES JEUX DE DONNÉES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCorpus = \"./ressources/AFDpresidentutf8/corpus.tache1.learn.utf8\"\n",
    "testCorpus = \"./ressources/AFDpresidentutf8/corpus.tache1.test.utf8\"\n",
    "\n",
    "alltxts_train, alllabs_train = load_pres(trainCorpus)\n",
    "alltxts_test, alllabs_test = load_pres(testCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des données brutes\n",
    "print(len(alltxts_train),len(alllabs_train))\n",
    "print(alltxts_train[:3])\n",
    "print(alllabs_train[:3])\n",
    "print(alltxts_train[-1])\n",
    "print(alllabs_train[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permet de compter l'effectif de chaque valeur unique dans une liste\n",
    "counter_train = Counter(alllabs_train)\n",
    "counter_test = Counter(alllabs_test)\n",
    "\n",
    "print(\"Nombre de phrases du corpus d'entraînement : \", len(alltxts_train))\n",
    "print(\"----> # de l'ancien président français Jacques Chirac (label '1') : \", counter_train[1])\n",
    "print(\"----> # de l'ancien président français François Mitterand (label '0') : \", counter_train[0])\n",
    "\n",
    "print(\"Nombre de phrases du corpus de test : \", len(alllabs_test))\n",
    "print(\"----> # de l'ancien président français Jacques Chirac (label '1') : \", counter_test[1])\n",
    "print(\"----> # de l'ancien président français François Mitterand (label '0') : \", counter_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut remarquer que les **classes sont assez déséquilibrées**. En effet, dans le corpus d'entraînement, seulement 13% des phrases ont été prononcées par François Mitterand et 10% pour le corpus de test. Cela peut avoir amener des algorithmes à mieux identifier les phrases de Mr Chirac que ceux de Mr Mitterand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING DES DONNÉES BRUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application du preprocessing à mon texte d'apprentissage et de test\n",
    "alltxts_train_prep = []\n",
    "alltxts_test_prep = []\n",
    "\n",
    "for train_sentence in alltxts_train:\n",
    "    alltxt_prep = preprocess(train_sentence)\n",
    "    alltxts_train_prep += [alltxt_prep]\n",
    "\n",
    "for test_sentence in alltxts_test:\n",
    "    alltxt_prep = preprocess(test_sentence)\n",
    "    alltxts_test_prep += [alltxt_prep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 premières lignes du corpus d'entraînement pré-traité\n",
    "alltxts_train_prep[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Première partie : BAG OF WORDS\n",
    "<span style=\"color:black\"> Les <b> bags of words (BoW) </b> sont des représentations textuelles utilisées pour transformer des textes en vecteurs numériques, où chaque dimension correspond à un mot unique dans un vocabulaire, et la valeur indique le nombre d'occurences de ce mot dans le texte. Cette méthode simple est largement utilisée pour de la classification de texte comme c'est le cas ici. Cependant, cette technique a quelques limites notamment celle <u> de ne pas prendre en compte la position des mots ni les relations sémantiques entre eux </u>. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permet de transformer chaque ligne en embedding type \"one-hot-encoding\"\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep = vectorizer.fit_transform(alltxts_train_prep)\n",
    "X_noprep = vectorizer.fit_transform(alltxts_train)\n",
    "print(vectorizer.get_feature_names_out()) #tous les mots différents du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "nb_clf = MultinomialNB()\n",
    "fit_nb_prep = nb_clf.fit(X_prep, alllabs_train)\n",
    "fit_nb_noprep = nb_clf.fit(X_noprep, alllabs_train)\n",
    "\n",
    "#Logistic Regression\n",
    "lr_clf = LogisticRegression(random_state=0, solver='lbfgs',n_jobs=-1)\n",
    "fit_lr_prep = lr_clf.fit(X_prep, alllabs_train)\n",
    "fit_lr_noprep = lr_clf.fit(X_noprep, alllabs_train)\n",
    "\n",
    "#Linear SVM\n",
    "svm_clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "fit_svm_prep = svm_clf.fit(X_prep, alllabs_train)\n",
    "fit_svm_noprep = svm_clf.fit(X_noprep, alllabs_train)\n",
    "\n",
    "#Labels vrais\n",
    "true = alllabs_test\n",
    "\n",
    "#Morceaux de discours avec application des fonctions de préprocessing puis sans\n",
    "test_corpus_prep = alltxts_test_prep\n",
    "test_corpus_noprep = alltxts_test\n",
    "\n",
    "#Application du one-hot-encoding à chaque morceau de discours\n",
    "X_test_prep = vectorizer.transform(test_corpus_prep)\n",
    "X_test_noprep = vectorizer.transform(test_corpus_noprep)\n",
    "\n",
    "#Prédictions des labels\n",
    "pred_nb_prep = nb_clf.predict(X_test_prep)\n",
    "pred_nb_noprep = nb_clf.predict(X_test_noprep)\n",
    "\n",
    "pred_lr_prep = lr_clf.predict(X_test_prep)\n",
    "pred_lr_noprep = lr_clf.predict(X_test_noprep)\n",
    "\n",
    "pred_svm_prep = svm_clf.predict(X_test_prep)\n",
    "pred_svm_noprep = svm_clf.predict(X_test_noprep)\n",
    "\n",
    "#Calcul de l'accuracy pour chaque modèle avec et sans préprocessing\n",
    "acc_nb_prep = accuracy_score(true, pred_nb_prep)\n",
    "acc_nb_noprep = accuracy_score(true, pred_nb_noprep)\n",
    "\n",
    "acc_lr_prep = accuracy_score(true, pred_lr_prep)\n",
    "acc_lr_noprep = accuracy_score(true, pred_lr_noprep)\n",
    "\n",
    "acc_svm_prep = accuracy_score(true, pred_svm_prep)\n",
    "acc_svm_noprep = accuracy_score(true, pred_svm_noprep)\n",
    "\n",
    "#Affichage des accuracy pour chaque modèle avec et sans préprocessing\n",
    "print(f\"Naïve Bayes accuracy with preprocessing: {acc_nb_prep}\")\n",
    "print(f\"Naïve Bayes accuracy without preprocessing: {acc_nb_noprep}\")\n",
    "\n",
    "print(f\"Logistic Regression accuracy with preprocessing: {acc_lr_prep}\")\n",
    "print(f\"Logistic Regression accuracy without preprocessing: {acc_lr_noprep}\")\n",
    "\n",
    "print(f\"SVM accuracy with preprocessing: {acc_svm_prep}\")\n",
    "print(f\"SVM accuracy without preprocessing: {acc_svm_noprep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Représentation graphique des accuracy avec pré-processing et sans preprocessing\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "X = ['Naïve Bayes', 'Logistic Regression', 'SVM']\n",
    "\n",
    "prep = [acc_nb_prep, acc_lr_prep, acc_svm_prep]\n",
    "no_prep = [acc_nb_noprep, acc_lr_noprep, acc_svm_noprep]\n",
    "\n",
    "X_axis = np.arange(len(X)) \n",
    "  \n",
    "plt.bar(X_axis - 0.2, prep, 0.4, label = 'Avec pré-processing') \n",
    "plt.bar(X_axis + 0.2, no_prep, 0.4, label = 'Sans pré-processing') \n",
    "\n",
    "plt.xticks(X_axis, X) \n",
    "plt.legend() \n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title(\"Taux de bonne classification pour chaque algorithme \\n selon si pré-processing des données ou non\", pad = 10)\n",
    "ax.set_ylim(0.5,1)\n",
    "\n",
    "#plt.show() #Affichage commenté pour fixer les derniers résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image originale :**\n",
    "![](images/barplot_preprocessing_or_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"> <u> Conclusion : </u> On remarque que le taux de bonne classification des morceaux de discours des 2 anciens présidents de la république est bon. En effet, on obtient des valeurs d'accuracy compris entre entre 0.78 et 0.85. Cependant, on remarque que la régression logistique a de légères meilleures performances que ce soit avec ou sans le pré-processing des données brutes. Concernant, ce pré-processing, on voit que pour les 3 algorithmes utilisés, le pré-processing améliore subtilement les résultats avec une augmentation de 0.03 points en moyenne.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ème partie : Usage d'un modèle de Transformers\n",
    "<span style=\"color:black\"> Les <b> Transformers </b> sont des modèles d'apprentissage profond utilisés principalement pour le traitement du langage naturel. Ces types de modèles utilisent un <u> mécanisme d'attention </u> pour traiter simultanément toutes les parties d'une séquence. Cela permet d'analyser efficacement des relations à longue distance dans les données. Ainsi, les Transformers sont particulièrement performants pour des tâches comme de la classification de texte. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser un tokenizer WordPiece\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "#taille du vocabulaire\n",
    "VOC_SIZE = 10000\n",
    "#taille des batch\n",
    "BATCH_SIZE = 32\n",
    "MAX_CHAR_SIZE = 1000\n",
    "NB_DOC_MAX = 12500 # par classe\n",
    "\n",
    "# Définir les pré-traitements\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Créer un entraîneur avec un vocabulaire cible\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=10000,  # Limite du vocabulaire\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# Entraîner le tokenizer sur vos données\n",
    "tokenizer.train_from_iterator(alltxts_train_prep, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = tokenizer.encode(\"[PAD]\").ids[0]\n",
    "print(\"PAD\",PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fabrication de tous les codes\n",
    "allcodes = [torch.tensor(tokenizer.encode(\"[CLS] \" + p).ids) for p in alltxts_train_prep]\n",
    "allcodes_test = [torch.tensor(tokenizer.encode(\"[CLS] \" + p).ids) for p in alltxts_test_prep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TextDataset(allcodes,alllabs_train)\n",
    "ds_test  = TextDataset(allcodes_test,alllabs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encapsulation des tokens dans un format DataLoader\n",
    "train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
    "test_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modèle de Transformers\n",
    "class TransSent(nn.Module):\n",
    "    \"\"\"\n",
    "    Modèle Transformer pour la classification de texte\n",
    "\n",
    "    Args:\n",
    "        nn.Module (module): module \"Module\" de Pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, voc_size, num_layers, num_heads, hidden_size_mlp, output_size, maxlen=1000):\n",
    "        super(TransSent, self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        # 1. Embedding de chaque mot\n",
    "        self.emb = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        # 2. Transformer Encoder Layer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_size, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_size_mlp,\n",
    "            activation='relu'\n",
    "        )\n",
    "\n",
    "        # 3. Transformer Encoder\n",
    "        self.trans = nn.TransformerEncoder(\n",
    "            self.encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # 4. Ajout des embeddings positionnels (sinusoïdaux)\n",
    "        self.register_buffer(\"posemb\", generate_sinusoidal_embeddings(maxlen, self.emb_size).unsqueeze(1))\n",
    "\n",
    "        # 5. Couche de classification (depuis le vecteur [CLS])\n",
    "        self.h2o = nn.Linear(emb_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "        maxlen = input.size(0)\n",
    "\n",
    "        padding_mask = (input[:, :] == PAD).T \n",
    "\n",
    "        # 1. translation of the input from int to emb + ajout des positional embeddings\n",
    "        xemb = self.emb(input) \n",
    "        xemb += self.posemb[:maxlen,:,:]\n",
    "        \n",
    "        # 2. Passage dans le transformer... Avec le masque pour le padding\n",
    "        encoded_output = self.trans(xemb, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # 3. Appliquer la classification sur le CLS\n",
    "        output = self.h2o(encoded_output[0,:,:]).squeeze(0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose hidden size\n",
    "emb_size = 128\n",
    "voc_size = 10000\n",
    "num_layers = 1\n",
    "num_heads = 1\n",
    "hidden_size_mlp = 128\n",
    "output_size = 2\n",
    "# build network\n",
    "transf = TransSent( emb_size, voc_size, num_layers, num_heads, hidden_size_mlp , output_size)\n",
    "transf.name = \"TransSent-\"+time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training du transformers\n",
    "n_epoch = 15\n",
    "train_list, test_list = train(transf, n_epoch, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À la fin de l'entraînement, on trace le graphique de l'accuracy\n",
    "plt.plot(range(n_epoch), train_list, label='Train Accuracy')\n",
    "plt.plot(range(0, n_epoch, 2), test_list, label='Test Accuracy')  # Afficher l'accuracy de test tous les 2 epochs\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Évolution des performances de classification \\n du transformers au cours des époques')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image originale :**\n",
    "![](images/perf_transformers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"> <u> Conclusion : </u> On remarque que le taux de bonne classification des morceaux de discours des 2 anciens présidents de la république en utilisant un transformers est bon aussi. En effet, on a au minimum 80% des discours qui ont été bien classés en appliquant le pré-processing sur les données brutes. Cependant, on remarque un grand écart entre l'accuracy du jeu d'entraînement et du jeu de test. On pourrait donc émettre l'hypothèse d'un surapprentissage du modèle. Pour éviter le surapprentissage, il est possible de faire de la régularisation en appliquant la technique dropout ou en augmentant les poids des observations mal classées.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3ème partie : Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"> Ollama est une plateforme permettant de télécharger et exécuter localement des modèles de langage open-source optimisé. Parmi les modèles proposés, il y a Qwen qui est réputé pour être un bon équilibre entre la performance et le poids. Pour évaluer la performance du modèle de langage Qwen sur cette tâche, on va utiliser 2 méthodes différentes appelées \"zero shot\" et \"few shot\". Le premier consiste à réaliser de la classification sans exemple et le second, de la classification avec quelques exemples. On va analyser s'il y a des différences de performances ou pas à l'aide de la fonction \"test_perf\". Cette dernière va séparer les réponses du prompt selon si elles sont bien classifiées (réponse et bon label associé), selon si elles ont un problème de format de réponse (\"cette phrase a été prononcée à par Chirac\" au lieu de \"Chirac\" seulement), enfin, selon si elles sont mal classifées (réponse et mauvais label associé). </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lancement de la classification du modèle Qwen en mode \"zero shot\" et \"few shot\"\n",
    "rep_0shot = classification_zero_shot(alltxts_train[115])\n",
    "print(alllabs_train[115])\n",
    "print(rep_0shot.response)\n",
    "\n",
    "rep_fewshot = classification_few_shot(alltxts_train[115])\n",
    "print(alltxts_train[115])\n",
    "print(rep_fewshot.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200 #seulement un échantillon est sélectionné pour accélerer le temps de traitement\n",
    "\n",
    "#Taux d'accuracy classification zero shot modèle ollama\n",
    "o1,p1,n1 = test_perf(alltxts_test_prep[:n], classification_zero_shot, alllabs_test[:n])\n",
    "print(o1,p1,n1)\n",
    "\n",
    "perc_o1 = 100*o1/(len(alltxts_test_prep[:n]))\n",
    "perc_p1 = 100*p1/(len(alltxts_test_prep[:n]))\n",
    "perc_n1 = 100*n1/(len(alltxts_test_prep[:n]))\n",
    "\n",
    "print(\n",
    "    'Zero-shot\\n ok : ', perc_o1, '%', \n",
    "    'Nuls : ', perc_n1, '%', \n",
    "    'Mauvais format de réponse : ', perc_p1, '%.')\n",
    "\n",
    "#Taux d'accuracy classification few shot modèle ollama\n",
    "o2,p2,n2 = test_perf(alltxts_test_prep[:n], classification_few_shot, alllabs_test[:n])\n",
    "\n",
    "perc_o2 = 100*o2/(len(alltxts_test_prep[:n]))\n",
    "perc_p2 = 100*p2/(len(alltxts_test_prep[:n]))\n",
    "perc_n2 = 100*n2/(len(alltxts_test_prep[:n]))\n",
    "\n",
    "print(\n",
    "    'Few-shot\\n ok : ', perc_o2, '%', \n",
    "    'nuls : ', perc_n2, '%', \n",
    "    'Mauvais format de réponse : ', perc_p2, '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Représentation graphique des accuracy avec la méthode zero shot et few shot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "X = ['Bonne \\n classification', 'Mauvais format \\n de réponse', 'Nuls']\n",
    "\n",
    "zero_shot = [perc_o1, perc_p1, perc_n1]\n",
    "few_shot = [perc_o2, perc_p2, perc_n2]\n",
    "\n",
    "X_axis = np.arange(len(X)) \n",
    "  \n",
    "plt.bar(X_axis - 0.2, zero_shot, 0.4, label = 'zero shot') \n",
    "plt.bar(X_axis + 0.2, few_shot, 0.4, label = 'few shot') \n",
    "\n",
    "plt.xticks(X_axis, X) \n",
    "plt.legend() \n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title(\n",
    "    \"Taux de bonne classification, de bonne classification mais mauvais format de réponse \\n et de réponses nulles du modèle Qwen selon l'usage de la méthode 'zero shot' ou 'few shot'\", \n",
    "    pad = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image originale :**\n",
    "![](images/ollama_discours.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"> <u> Conclusion : </u> On remarque que le taux de bonne classification des morceaux de discours des 2 anciens présidents de la république en utilisant le modèle Qwen d'Ollama est drastiquement plus élevé en donnant quelques exemples au modèle (méthode \"few-shot\") que sans (méthode \"one-shot\"). En effet, on passe d'un score de 30% environ à quasiment 60% de phrases associées à son bon auteur. Également, le taux de mauvais format de réponse diminue en utilisant la méthode \"few-shot\" en passant de 20% de mauvaise classification avec la méthode \"zero-shot\" à 10% environ. Concernant les phrases mal classées par le modèle, on passe de plus de 50% avec la méthode zero-shot (elle est par ailleurs la classe majoritaire avec cette dernière méthode) à 30% environ. \n",
    "Finalement, le taux de discours bien classés reste majoritaires lorsque le modèle de language reçoit quelques exemples d'entraînement, contrairement à la méthode \"zero-shot\" où il y a 1 phrases sur 2 mal classée environ.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion générale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\">Au final, l'exercice de classer correctement des phrases provenant de discours de 2 anciens présidents de la République française, à partir de différentes méthodes d'apprentissages supervisées ou non, montrent plusieurs choses.\n",
    "- D'abord, les méthodes bag-of-words et transformers montrent de bonnes performances de classification des discours et plutôt équivalentes même s'il y a de l'over-fitting apparent pour le modèle transformers. Il pourrait donc avoir le potentiel de faire mieux. \n",
    "- De plus, on s'est rendu compte que le pré-traitement des données brutes est essentiel pour le gain de performances de classification. \n",
    "- Enfin, concernant le modèle Qwen d'Ollama, les méthodes \"zero-shot\" et \"few-shot\" nous enseignent que Qwen améliore considérablement ses performances avec l'entraînement au préalable sur quelques exemples (méthode \"few-shots\"). La prise en compte de certaines erreurs d'orthographe ou de majuscules avec une expression régulière a également aidé à améliorer la précision de classification du modèle. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
